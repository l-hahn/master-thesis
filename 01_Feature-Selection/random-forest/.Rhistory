MakeSynthData = function(XData){
ColSample = function(Col){
sample(Col,length(Col),replace = T)
}
SynthData = rbind(XData,apply(XData,2,ColSample))
SynthData$Label= as.factor(c(rep(0,nrow(XData)),rep(1,nrow(XData))))
return(SynthData)
}
library(doParallel)
library(bigrf)
registerDoParallel(cores=12)
vars = 1:4
SynthData = MakeSynthData(iris[-5])
forest1 = bigrfc(SynthData,SynthData$Label,ntree=500L, varselect=vars)
varimp(forest1,SynthData[-5])
MakeSynthData = function(XData){
ColSample = function(Col){
sample(Col,length(Col),replace = T)
}
SynthData = rbind(XData,apply(XData,2,ColSample))
SynthData$Label= as.factor(c(rep(0,nrow(XData)),rep(1,nrow(XData))))
return(SynthData)
}
library(doParallel)
library(bigrf)
registerDoParallel(cores=12)
vars = 1:4
SynthData = MakeSynthData(iris[-5])
forest1 = bigrfc(SynthData,SynthData$Label,ntree=500L, varselect=vars)
varimp(forest1,SynthData[-5])
Noten = c(1.3, 1.3,2.0,2.3,2.3,
1.3,1.7,1.0, 1.0, 3.3, 1.3,1.0)
Credits = c(5,6,5,6,6,
6,6,6, 3, 6, 6,6)
sum(Noten*Credits)/sum(Credits)
Noten = c(1.3,1.0, 1.3,2.0,2.3,2.3, 1.3
1.3,1.7,1.0, 1.0, 3.3, 1.3,1.0,
1.0)
Credits = c(5,6, 6,5,6,6, 6,
6,6,6, 3, 6, 6,6,
30)
sum(Noten*Credits)/sum(Credits)
Noten = c(1.3,1.0, 1.3,2.0,2.3,2.3, 1.3
1.3,1.7,1.0, 1.0, 3.3, 1.3,1.0,
1.0)
Credits = c(5,6, 6,5,6,6, 6,
6,6,6, 3, 6, 6,6,
30)
sum(Noten*Credits)/sum(Credits)
Noten
Credits
lengt(Noten)
length(Noten)
length(Credits)
Noten
Credits
Noten = c(1.3,1.0, 1.3,2.0,2.3,2.3, 1.3,
1.3,1.7,1.0, 1.0, 3.3, 1.3,1.0,
1.0)
Credits = c(5,6, 6,5,6,6, 6,
6,6,6, 3, 6, 6,6,
30)
sum(Noten*Credits)/sum(Credits)
Noten = c(1.3,1.0, 1.3,2.0,2.3,2.3,
1.3,1.7,1.0, 1.0, 3.3, 1.3,1.0,
1.0)
Credits = c(5,6, 6,5,6,6,
6,6,6, 3, 6, 6,6,
30)
sum(Noten*Credits)/sum(Credits)
Diff
library(keras)
LRT <- read.table('monotonicDEG')
setwd("~/Schreibtisch/Machine_Learning/Tomato-RNA_K-depletion/")
LRT <- read.table('monotonicDEG')
LRTt = t(LRT)
View(LRT)
TrainIdx = c(1,2,4,5,7,8,10,11)
TrainType = c(0,0,1,1,2,2,5,5)
TestIdx = c(3,6,9,12)
TestType = c(0,1,2,5)
x_train = LRTt[TrainIdx,]
y_train = c(0,0,1,1,2,2,3,3)
x_test = LRTt[TestIdx,]
y_test = c(0,1,2,3)
dim(x_train)
class(x_train)
x_train_array = array(x_train,dim = c(nrow(x_train),ncol(x_train)))
y_train_onehot = to_categorical(y_train)
y_train_onehot
feedforward_model1 = keras_model_sequential() %>%
layer_dense(units=100,
activation = "relu",
input_shape = dim(x_train_array)[2]) %>%
layer_dense(units = ncol(x_train)/2,
activation = "relu") %>%
layer_dense(units=15,
activation = "relu") %>%
layer_dense(units=5,
activation = "softmax")
feedforward_model1
feedforward_model1 = keras_model_sequential() %>%
layer_dense(units=100,
activation = "relu",
input_shape = dim(x_train_array)[2]) %>%
layer_dense(units = ncol(x_train)/2,
activation = "relu") %>%
layer_dense(units=13,
activation = "relu") %>%
layer_dense(units=4,
activation = "softmax")
feedforward_model1
feedforward_model1 %>% compile(optimizer="rmsprop",
loss = "categorical_crossentropy",
metric = "acc")
history = feedforward_model1 %>% fit(x = x_train_array,
y = y_train_onehot,
batch_size = 1,
validation_split = 0.05,
epochs = 10)
x_train
LRTt
View(LRT)
rownames(LRT)
rownames(LRT) LRT$gene
rownames(LRT) = LRT$gene
rownames(LRT)
LRT = LRT[-1]
LRT
LRTt = t(LRT)
TrainIdx = c(1,2,4,5,7,8,10,11)
TrainType = c(0,0,1,1,2,2,5,5)
TestIdx = c(3,6,9,12)
TestType = c(0,1,2,5)
x_train = LRTt[TrainIdx,]
y_train = c(0,0,1,1,2,2,3,3)
x_test = LRTt[TestIdx,]
y_test = c(0,1,2,3)
x_train
dim(x_train)
class(x_train)
x_train_array = array(x_train,dim = c(nrow(x_train),ncol(x_train)))
y_train_onehot = to_categorical(y_train)
feedforward_model1 = keras_model_sequential() %>%
layer_dense(units=100,
activation = "relu",
input_shape = dim(x_train_array)[2]) %>%
layer_dense(units = ncol(x_train)/2,
activation = "relu") %>%
layer_dense(units=13,
activation = "relu") %>%
layer_dense(units=4,
activation = "softmax")
feedforward_model1
feedforward_model1 %>% compile(optimizer="rmsprop",
loss = "categorical_crossentropy",
metric = "acc")
history = feedforward_model1 %>% fit(x = x_train_array,
y = y_train_onehot,
batch_size = 1,
validation_split = 0.05,
epochs = 10)
feedforward_model1 = keras_model_sequential() %>%
layer_dense(units=100,
activation = "sigmoid",
input_shape = dim(x_train_array)[2]) %>%
layer_dense(units = ncol(x_train)/2,
activation = "relu") %>%
layer_dense(units=13,
activation = "relu") %>%
layer_dense(units=4,
activation = "softmax")
feedforward_model1
feedforward_model1 %>% compile(optimizer="rmsprop",
loss = "categorical_crossentropy",
metric = "acc")
history = feedforward_model1 %>% fit(x = x_train_array,
y = y_train_onehot,
batch_size = 1,
validation_split = 0.05,
epochs = 10)
predModel = feedforward_model %>% predict(x_test)
predModel = feedforward_model1 %>% predict(x_test)
predModel
inverse_to_classes = function(mat){
apply(mat,1,function(row) which(row==max(row))-1)
}
FFN_CM = table(inverse_to_classes(predModel),y_test)
FFN_CM
predModel
y_test
feedforward_model1 = keras_model_sequential() %>%
layer_dense(units=100,
activation = "sigmoid",
input_shape = dim(x_train_array)[2]) %>%
layer_dense(units = ncol(x_train)/2,
activation = "relu") %>%
layer_dense(units=13,
activation = "relu") %>%
layer_dense(units=4,
activation = "softmax")
feedforward_model1
feedforward_model1 %>% compile(optimizer="rmsprop",
loss = "categorical_crossentropy",
metric = "acc")
history = feedforward_model1 %>% fit(x = x_train_array,
y = y_train_onehot,
batch_size = 1,
validation_split = 0.05,
epochs = 10)
predModel = feedforward_model1 %>% predict(x_test)
predModel
predModel
y_test
FFN_CM = table(inverse_to_classes(predModel),y_test)
FFN_CM
feedforward_model1 = keras_model_sequential() %>%
layer_dense(units=500,
activation = "sigmoid",
input_shape = dim(x_train_array)[2]) %>%
layer_dense(units = ncol(x_train)/2,
activation = "relu") %>%
layer_dense(units=13,
activation = "relu") %>%
layer_dense(units=4,
activation = "softmax")
feedforward_model1
feedforward_model1 %>% compile(optimizer="rmsprop",
loss = "categorical_crossentropy",
metric = "acc")
history = feedforward_model1 %>% fit(x = x_train_array,
y = y_train_onehot,
batch_size = 1,
validation_split = 0.05,
epochs = 10)
predModel = feedforward_model1 %>% predict(x_test)
predModel
predModel
y_test
FFN_CM = table(inverse_to_classes(predModel),y_test)
FFN_CM
setwd("~/Schreibtisch/master-thesis/01_Feature-Selection/random-forest/")
data = read.table("data.attr")
View(data)
plot(density(data$V2))
plot(density(log2(data$V2)))
plot(density(log(data$V2)))
plot(density(data$V2))
